{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9edb21-8901-4b98-92d5-89df6f5a272b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "from math import cos, asin, sqrt\n",
    "import re\n",
    "import time\n",
    "from datetime import date\n",
    "import traceback\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import glob\n",
    "from geocube.api.core import make_geocube\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import folium\n",
    "import datetime\n",
    "import time\n",
    "from folium import plugins\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "import contextily as cx\n",
    "from shapely.geometry import box\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2467d-15ad-4688-b7a7-5270b057ab3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tess' Goals\n",
    "\n",
    "I am interested in tying lighting strike data to individuals fires to contrsuct a timeline of when fires in 2023 Canada could have ignited compared to when satellites detected them. To do this, I have access to a high resolution dataset of global lightning strikes down to the millisecond. The data is stored as daily `.raw` files, each with global coverage and ~200000 rows. A public copy of the data is stored at `s3://maap-ops-workspace/shared/tmccabe/Lighting_data` (See Notes). I have been trying to find effecient ways to construct timeseries for specific regions out of these files. Ideally I would like to be able to write out a non-kernel-crashing file that is \"Canada-2032-Jan-Oct.csv\" or similar. \n",
    "\n",
    "\n",
    "Note:\n",
    "\n",
    "- Because I ran into \"Transport endpoint not conected\" errors at times during debugging, I also have a private copy of the data on /projects. \n",
    "- Based on advice, I also transformed each file into a `.parquet` file, also stored on /projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf91000-040e-4b4b-a1af-b5c24b091759",
   "metadata": {},
   "source": [
    "# Things Tess has Tried: For loop with frequent write-outs\n",
    "\n",
    "I tried writing a for loop that writes-out every 5 files. The Kernel crashes after ~5 files, certain files too big for memory at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d891904-e6f9-40f0-8872-fb88579322db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \"For Loop Style Read-in\" #####\n",
    "\n",
    "\n",
    "def get_ca_lt(date, bbox = [-83.69877641421793, 44.25483911637959, -55, 62.94135765648493]): #[-169, 44, -48, 75]\n",
    "    CA_bbox = box(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "    print(date)\n",
    "    \n",
    "    #lt = pd.read_csv(\"/projects/my-public-bucket/Lighting_data/2023/englnrt_\"+ date+\"_daily_v1_lit.raw\", names =[\"InterCloud\",\n",
    "    lt = pd.read_csv(\"/projects/2023_lightning_data/englnrt_\"+ date+\"_daily_v1_lit.raw\", names =[\"InterCloud\",\n",
    "                                                                                             \"t\", \n",
    "                                                                                             \"lat\", \n",
    "                                                                                             \"lon\",\n",
    "                                                                                             \"current_mag\", \n",
    "                                                                                             \"multiplicity_0\", \n",
    "                                                                                             \"accr\", \n",
    "                                                                                             \"error_elps\", \n",
    "                                                                                             \"num_station\"])\n",
    "    lt =  gpd.GeoDataFrame(lt, geometry=gpd.points_from_xy(lt['lon'], lt['lat']))\n",
    "    lt.t = lt.t.astype(\"datetime64[ns]\")\n",
    "    lt = lt[lt.InterCloud == 0]\n",
    "    lt = lt.set_crs(\"epsg:4326\")\n",
    "    smol = lt.clip(CA_bbox)\n",
    "    return(smol)\n",
    "\n",
    "def lt_timeseries_extract(date_snap, region, sub_region, run_name): ## putting into function because might help memory???\n",
    "    \n",
    "    run_name = str(date.today()) + run_name_st\n",
    "    lt = [] # full info per day\n",
    "    bad_ids = [] # list of IDs that failed due to error\n",
    "    bbox = [-83.69877641421793, 44.25483911637959, -55, 62.94135765648493]\n",
    "    for n,i in enumerate(date_snap, start = 0):\n",
    "\n",
    "        try:\n",
    "            smol = get_ca_lt(str(i), bbox = bbox )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error at date: \",n,)\n",
    "            bad_log = traceback.format_exc(limit = 40) # Get log of error\n",
    "            print(bad_log)\n",
    "            bad_ids.append(\n",
    "            {\n",
    "                \"date\": n, \n",
    "                \"log\": bad_log\n",
    "            }\n",
    "            )\n",
    "        lt.append(smol)\n",
    "        if((n%5 == 0) | (n == (len(date_range) -1))):\n",
    "            print(\"\")\n",
    "            manyfr = gpd.GeoDataFrame(pd.concat(lt, ignore_index=True), crs= smol.crs)\n",
    "            manyfr.to_csv(\"/projects/old_shared/fire_weather_vis/Lightning_analysis/computed_data/Lt_\" + region + \"_\" + sub_region +  \"_\" + run_name +\".csv\")\n",
    "            debug = pd.DataFrame(bad_ids)\n",
    "            debug.to_csv(\"/projects/old_shared/fire_weather_vis/Lightning_analysis/computed_data/DEBUG_\" + region + \"_\" + sub_region + \"_\" + run_name + \".csv\")\n",
    "        \n",
    "    return(manyfr)\n",
    "\n",
    "date_range = pd.date_range(start = \"2023-05-01 00:00:00\", end = \"2023-07-20 00:00:00\", freq=\"24H\") \n",
    "date_snap = date_range.strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "################# Horrible for loop ##################\n",
    "sub_region = \"example_Quebec\"\n",
    "region =  \"example_CA\"\n",
    "run_name_st = \"example\"\n",
    "manyfr = lt_timeseries_extract(date_snap = date_snap, region = region,  sub_region = sub_region, run_name = run_name_st) ## putting into function because might help memory???\n",
    "\n",
    "# Grab Currrent Time After Running the Code\n",
    "end = time.time()\n",
    "\n",
    "#Subtract Start Time from The End Time\n",
    "total_time = end - start\n",
    "print(\"\\n\"+ str(total_time)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db744d2-85b9-4798-88bd-9e18fa1502fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### When for loop kills Kernel and dies, I can read in all the partial write-outs\n",
    "\n",
    "def concat_subsets(files):\n",
    "    df = []\n",
    "    for f in files:\n",
    "        manyfr = pd.read_csv(f)\n",
    "\n",
    "        manyfr = gpd.GeoDataFrame(manyfr)\n",
    "\n",
    "        manyfr.t = manyfr.t.astype(\"datetime64[ns]\")\n",
    "        df.append(manyfr)\n",
    "    df = pd.concat(df)\n",
    "    return(df)\n",
    "\n",
    "files = glob.glob(\"/projects/old_shared/fire_weather_vis/Lightning_analysis/computed_data/Lt_CA_Quebec_*.csv\")\n",
    "\n",
    "manyfr= concat_subsets(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d3c62e-65a4-4aa7-b680-65b00b26e932",
   "metadata": {},
   "source": [
    "This has been my most successfull technique because it gives me output I can plot. It's horrible becuase it means I need to supervise it and wait for it to die, restart it with a new name, and wait again. It also doesn't work at all for some files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25faaf5d-b1ed-4ff2-b565-1fc07ea41d7c",
   "metadata": {},
   "source": [
    "# Things Tess has Tried: Multiprocess \n",
    "\n",
    "I've tried a python script using multiprocessing based on an example a colleague (Jordan Caraballo-Vega - Thanks for the help!) provided. It executed, but when I watched it with `top` I noticed that all the parallel sub-tests would be get allocated, and take up CPU and memory, but then slowly all the tasks would be paused until just one was running, and I assume ran until it got killed. Nothing was output. Here is the `.py` file: \n",
    "\n",
    "I moved to dask hoping to use it to diagnose what the computational limit multiprocessing was hitting was. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430de98a-1297-49f3-857f-24c91495ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from glob import glob\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "\n",
    "csvs_regex = '/projects/2023_lightning_data/*.raw'\n",
    "\n",
    "def read_gdf(filename):\n",
    "    af_df = pd.read_csv(filename, names =[\"InterCloud\",\n",
    "                                                                                             \"t\", \n",
    "                                                                                             \"lat\", \n",
    "                                                                                             \"lon\",\n",
    "                                                                                             \"current_mag\", \n",
    "                                                                                             \"multiplicity_0\", \n",
    "                                                                                             \"accr\", \n",
    "                                                                                             \"error_elps\", \n",
    "                                                                                             \"num_station\"])\n",
    "    # convert to GeoDataFrame\n",
    "    af_gdf = gpd.GeoDataFrame(\n",
    "        af_df,\n",
    "        geometry=gpd.points_from_xy(af_df.lon, af_df.lat),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    print(filename)\n",
    "    return af_gdf\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    csvs_list = glob(csvs_regex)\n",
    "    print(len(csvs_list))\n",
    "    start = time.time()\n",
    "    print(start)\n",
    "\n",
    "    with Pool(processes=(cpu_count()) - 20) as p:\n",
    "        result = p.map(read_gdf, csvs_list)\n",
    "    gdf = pd.concat(result)\n",
    "    print(gdf.shape)\n",
    "    gdf.to_file(\"/projects/old_shared/fire_weather_vis/Lightning_analysis/test_para/2023.gpkg\") # Nothing ever wrote to here\n",
    "    end = time.time()\n",
    "\n",
    "    total_time = end - start\n",
    "    print(\"\\n\"+ str(total_time))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37c488-62da-487c-a694-6c77e83ffc06",
   "metadata": {},
   "source": [
    "# Things Tess has Tried: Dask-delaying my exisitng functions\n",
    "\n",
    "I first tried to use dask delay on the functions I wrote for my for-loop. Once I figured out not to instantiate the client through the tab (Thanks Alex & Julia), I got garbage collection warnings for a while, until either the kernel crashed, or I lost patience. Again, nothing was written out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0d3a5-aed9-412a-944b-65c94b65da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client(threads_per_worker=2, n_workers=3) # I have also tried with 1 worker. Same results AFAIK. \n",
    "client\n",
    "\n",
    "lazy_results = []\n",
    "\n",
    "for d in date_snap:\n",
    "    tmp = dask.delayed(get_ca_lt)(str(d))\n",
    "    lazy_results.append(tmp)\n",
    "\n",
    "futes = dask.persist(*lazy_results) \n",
    "\n",
    "\n",
    "results = dask.compute(*futes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5a13e-ddfd-4a5a-9f21-58c0057a5d53",
   "metadata": {},
   "source": [
    "```\n",
    "2023-10-05 07:16:28,594 - distributed.utils_perf - WARNING - full garbage collections took 34% CPU time recently (threshold: 10%)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9b266-a799-4207-828c-f157f3488707",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Things Tess has Tried: dask.dataframe and dask_geopandas\n",
    "\n",
    "Thanks to anouther helpful suggestion + example by Jordan. I switched to using dask.dataframe and dask_geopandas. This seemed pretty close to working. It front-loaded the subsetting, let me read-in all my files at once, and even returned numbers for `ddf.head()` sometimes. However, sometimes when I did an intermediate `ddf.head()` or when I tried to plot the `day_strike` object, I just got garbage collection warnings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea83a62-e42f-4f05-8030-bb5222435cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas\n",
    "from glob import glob\n",
    "from shapely.geometry import box\n",
    "import dask\n",
    "\n",
    "\n",
    "\n",
    "from dask.distributed import Client, progress\n",
    "client = Client(threads_per_worker=2, n_workers=3)\n",
    "client\n",
    "\n",
    "def lt_file_subset(csvs_regex = '/projects/2023_lightning_data/e*.raw'):\n",
    "    \n",
    "    bbox = [-83.69877641421793, 44.25483911637959, -55, 62.94135765648493]\n",
    "    CA_bbox = box(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "    \n",
    "    ddf = dd.read_csv(csvs_regex, names =[\"InterCloud\",\n",
    "    \"t\", \n",
    "    \"lat\", \n",
    "    \"lon\",\n",
    "    \"current_mag\", \n",
    "    \"multiplicity_0\", \n",
    "    \"accr\", \n",
    "    \"error_elps\", \n",
    "    \"num_station\"], \n",
    "                      dtype={\n",
    "        'InterCloud': 'int', \n",
    "        'lat': 'str',\n",
    "        'lon': 'str', \n",
    "        'current_mag': 'float64',\n",
    "        'multiplicity_0': 'str', \n",
    "        'accr': 'float64',\n",
    "        'error_elps': 'float64',\n",
    "        'num_station': 'int'\n",
    "    })\n",
    "    ddf = ddf.set_geometry(\n",
    "            dask_geopandas.points_from_xy(ddf, 'lon', 'lat')\n",
    "    )\n",
    "\n",
    "    ddf = ddf.clip(CA_bbox)\n",
    "    ddf = ddf[ddf.InterCloud == 0]\n",
    "    ddf.t = ddf.t.astype(\"datetime64[ns]\")\n",
    "    ddf[\"t_agg\"] = ddf.t.dt.strftime(\"%Y-%m-%d %H:00:00\")\n",
    "    day_strike = ddf.groupby('t_agg').count() # Group into hourly resolution\n",
    "\n",
    "    return(day_strike)\n",
    "\n",
    "day_strike = lt_file_subset(csvs_regex = '/projects/2023_lightning_data/e*.raw')\n",
    "\n",
    "plt.plot(day_strike.index.astype(\"datetime64[ns]\"), day_strike.InterCloud) ## just garbage collection for a while"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fireatlas_oct4_2)",
   "language": "python",
   "name": "fireatlas_oct4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
